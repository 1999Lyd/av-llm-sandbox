{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(\n",
    "    # KEEP IT PRIVATE!\n",
    "    api_key=\"sk-***\",\n",
    ")\n",
    "max_retries = 5\n",
    "retry_delay = 2.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'dataset_train.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    data = file.read()\n",
    "index_dict = json.loads(data)\n",
    "dataset = index_dict['dataset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def flatten_frames(src_dataset):\n",
    "    \"\"\"\n",
    "    Return a list of dicts — one per frame — with keys:\n",
    "        frame_id, meta_action (str), waypoints_2d (str), image_paths (dict)\n",
    "    Scene/agent IDs are discarded.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    for scene in src_dataset.values():\n",
    "        for agent in scene.values():\n",
    "            for fid, frame in agent.items():\n",
    "                # ---------- 1) meta_action  ----------\n",
    "                lat_vals = []\n",
    "                lon_vals = []\n",
    "                for rec in frame[\"meta_actions\"].values():\n",
    "                    if rec is not None:\n",
    "                        lat, lon = rec.get(\"lateral\"), rec.get(\"longitudinal\")\n",
    "                    lat_vals.append(lat)\n",
    "                    lon_vals.append(lon)\n",
    "                    \n",
    "\n",
    "                majority_lat = Counter(lat_vals).most_common(1)[0][0] if lat_vals else None\n",
    "                majority_lon = Counter(lon_vals).most_common(1)[0][0] if lon_vals else None\n",
    "                meta_action_str = str([majority_lat, majority_lon])\n",
    "\n",
    "                # ---------- 2) waypoints_2d  ----------\n",
    "                tuples = []\n",
    "                # sort by the integer part of 'dt_X'\n",
    "                for k in sorted(frame[\"waypoints_3d\"],\n",
    "                                key=lambda s: int(s.split('_')[1])):\n",
    "                    if frame[\"waypoints_3d\"][k] is not None:\n",
    "                        x, _, z = frame[\"waypoints_3d\"][k]\n",
    "                        tuples.append((round(x, 1), round(z, 1)))\n",
    "                waypoints_str = str(tuples)\n",
    "                \n",
    "                speeds = [st[\"speed\"] for st in frame[\"agent_state\"].values()\n",
    "                          if \"speed\" in st]\n",
    "                speed_val = round(speeds[0],1) if speeds else None\n",
    "                # ---------- 3) pack result ----------\n",
    "                out.append({\n",
    "                    \"frame_id\":      fid,\n",
    "                    \"image_paths\":   frame[\"image_paths\"],  # untouched\n",
    "                    \"meta_action\":   meta_action_str,\n",
    "                    \"waypoints_2d\":  waypoints_str,\n",
    "                    \"speed\": speed_val,\n",
    "                })\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frame_id': 'frame_8',\n",
       " 'image_paths': {'CAM_BACK': '../data/nuscenes-full/samples/CAM_BACK/n015-2018-07-18-11-18-34+0800__CAM_BACK__1531884160887525.jpg',\n",
       "  'CAM_BACK_LEFT': '../data/nuscenes-full/samples/CAM_BACK_LEFT/n015-2018-07-18-11-18-34+0800__CAM_BACK_LEFT__1531884160897423.jpg',\n",
       "  'CAM_BACK_RIGHT': '../data/nuscenes-full/samples/CAM_BACK_RIGHT/n015-2018-07-18-11-18-34+0800__CAM_BACK_RIGHT__1531884160877893.jpg',\n",
       "  'CAM_FRONT': '../data/nuscenes-full/samples/CAM_FRONT/n015-2018-07-18-11-18-34+0800__CAM_FRONT__1531884160862460.jpg',\n",
       "  'CAM_FRONT_LEFT': '../data/nuscenes-full/samples/CAM_FRONT_LEFT/n015-2018-07-18-11-18-34+0800__CAM_FRONT_LEFT__1531884160854844.jpg',\n",
       "  'CAM_FRONT_RIGHT': '../data/nuscenes-full/samples/CAM_FRONT_RIGHT/n015-2018-07-18-11-18-34+0800__CAM_FRONT_RIGHT__1531884160870339.jpg'},\n",
       " 'meta_action': \"['VEER_RIGHT', 'DECEL']\",\n",
       " 'waypoints_2d': '[(0.3, 11.5), (1.0, 20.5), (2.8, 27.7)]',\n",
       " 'speed': 7.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outset = flatten_frames(dataset)\n",
    "outset[167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scene_id': 'f0f120e4d4b0441da90ec53b16ee169d', 'scene_description': 'The ego vehicle proceeds through the intersection, continuing along the current roadway.', 'frame_id': '4a0798f849ca477ab18009c3a20b7df2', 'objects': {'objects': [{'object_key': '<c1,CAM_BACK,1088.3,497.5>', 'Category': 'Vehicle', 'Status': 'Moving', 'Visual_description': 'Brown SUV.', '2d_bbox': [966.6, 403.3, 1224.1, 591.7]}, {'object_key': '<c2,CAM_BACK,864.2,468.3>', 'Category': 'Vehicle', 'Status': 'Moving', 'Visual_description': 'Black sedan.', '2d_bbox': [816.7, 431.6, 917.2, 505.0]}, {'object_key': '<c3,CAM_FRONT,1043.2,82.2>', 'Category': 'Traffic element', 'Status': None, 'Visual_description': 'Green light.', '2d_bbox': [676.4, 0.0, 1452.6, 171.5]}], 'image_path': {'CAM_FRONT': '../nuscenes/samples/CAM_FRONT/n008-2018-09-18-13-10-39-0400__CAM_FRONT__1537291010612404.jpg', 'CAM_FRONT_LEFT': '../nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-13-10-39-0400__CAM_FRONT_LEFT__1537291010604799.jpg', 'CAM_FRONT_RIGHT': '../nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-13-10-39-0400__CAM_FRONT_RIGHT__1537291010620482.jpg', 'CAM_BACK': '../nuscenes/samples/CAM_BACK/n008-2018-09-18-13-10-39-0400__CAM_BACK__1537291010637558.jpg', 'CAM_BACK_LEFT': '../nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-13-10-39-0400__CAM_BACK_LEFT__1537291010647405.jpg', 'CAM_BACK_RIGHT': '../nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-13-10-39-0400__CAM_BACK_RIGHT__1537291010628113.jpg'}}}\n"
     ]
    }
   ],
   "source": [
    "def extract_cam_front_objects(data_dict):\n",
    "    \"\"\"\n",
    "    For each scene in data_dict, collect only the objects whose keys\n",
    "    contain 'CAM_FRONT'. Return a list of dictionaries, one per scene:\n",
    "    \n",
    "    {\n",
    "      'scene_id': ...,\n",
    "      'scene_description': ...,\n",
    "      'objects': [\n",
    "          {\n",
    "            'object_key': ...,\n",
    "            'Category': ...,\n",
    "            'Status': ...,\n",
    "            'Visual_description': ...,\n",
    "            '2d_bbox': ...\n",
    "          },\n",
    "          ...\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    scenes_output = []\n",
    "\n",
    "    for scene_id, scene_data in data_dict.items():\n",
    "        \n",
    "        scene_desc = scene_data.get(\"scene_description\", \"N/A\")\n",
    "        key_frames = scene_data.get(\"key_frames\", {})\n",
    "        # We'll gather only front-camera objects in this list\n",
    "        for frame_id, frame_data in key_frames.items():\n",
    "            key_object_infos = frame_data.get(\"key_object_infos\", {})\n",
    "            image_paths = frame_data.get(\"image_paths\", {})\n",
    "            cam_front_objects = {}\n",
    "            cam_front_objects['objects'] = []\n",
    "            cam_front_objects['image_path'] = image_paths\n",
    "            for obj_key, obj_info in key_object_infos.items():\n",
    "                \n",
    "                category = obj_info.get(\"Category\", \"Unknown\")\n",
    "                status = obj_info.get(\"Status\", \"Unknown\")\n",
    "                visual_desc = obj_info.get(\"Visual_description\", \"\")\n",
    "                bbox = obj_info.get(\"2d_bbox\", [])\n",
    "\n",
    "                cam_front_objects[\"objects\"].append({\n",
    "                    \"object_key\": obj_key,\n",
    "                    \"Category\": category,\n",
    "                    \"Status\": status,\n",
    "                    \"Visual_description\": visual_desc,\n",
    "                    \"2d_bbox\": bbox\n",
    "                    # Note: we are NOT including 'frame_id' here\n",
    "                })\n",
    "            scene_entry = {\n",
    "                \"scene_id\": scene_id,\n",
    "                \"scene_description\": scene_desc,\n",
    "                \"frame_id\": frame_id,\n",
    "                \"objects\": cam_front_objects\n",
    "            }\n",
    "            \n",
    "            scenes_output.append(scene_entry)\n",
    "        \n",
    "    \n",
    "    return scenes_output\n",
    "\n",
    "results = extract_cam_front_objects(index_dict)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(path_to_image: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads an image file from disk and returns a base64-encoded string (JPEG).\n",
    "    \"\"\"\n",
    "    with open(path_to_image, \"rb\") as f:\n",
    "        image_bytes = f.read()\n",
    "    return base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "def build_autonomous_driving_prompt(\n",
    "    camera_info_dict,\n",
    "    use_base64=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a text prompt for GPT by either inserting\n",
    "    base64-encoded images or textual descriptions.\n",
    "    \"\"\"\n",
    "    # Start with a short context\n",
    "    system_prompt = '''\n",
    "        You are an advanced autonomous driving assistant with expertise in scene understanding, object detection, and action planning.\n",
    "    '''\n",
    "\n",
    "    # The user message includes demonstration data + the new scenario\n",
    "    user_prompt = '''\n",
    "    Below is data from an autonomous driving scenario. You are provided with:\n",
    "    1) Six camera images (front-left, front, front-right, back-left, back, back-right).\n",
    "    2) A demonstration example showing how you should reason step-by-step and provide a final action.\n",
    "\n",
    "    Please follow this structure:\n",
    "    1) Summarize the detected objects and their statuses from the images.\n",
    "    2) Predict the future movement or intent of key objects.\n",
    "    3) Propose reasoning on the above content and give some corresponding potential safe and correct driving actions.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Demonstration Example\n",
    "\n",
    "    **Camera Views (Example)**:\n",
    "    1) front-left: [an image showing parked cars on the curb, no pedestrians].\n",
    "    2) front: [an image showing a blue sedan ~30m ahead, slight braking.\n",
    "    3) front-right: [an image showing clear sidewalk, no immediate obstacles].\n",
    "    4) back-left: [an image showing a black SUV approaching quickly in the left lane].\n",
    "    5) back: Clear, [an image showing no vehicle behind in the same lane].\n",
    "    6) back-right: [an image showing a bicycle rider moving in the right lane behind].\n",
    "\n",
    "    **Sample Step-by-Step Reasoning (Example)**:\n",
    "    1) Detected Objects & Status:\n",
    "    - Blue sedan ahead is braking slightly.\n",
    "    - Black SUV is behind in the adjacent lane, accelerating.\n",
    "    - Bicycle behind to the right, stable speed.\n",
    "    2) Future Movement Prediction:\n",
    "    - The sedan may slow further or maintain a slower speed.\n",
    "    - The SUV may attempt to pass or merge.\n",
    "    - The bicycle will continue along the right lane.\n",
    "    3) Action Planning:\n",
    "    - Since the sedan is slowing, be prepared to reduce speed.\n",
    "    - The SUV might merge, so keep safe distance and monitor left mirror.\n",
    "    - Maintain lane position and reduce speed to maintain a safe following distance.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Now Your Turn\n",
    "\n",
    "    Below is the new scenario for which we need the same type of reasoning. Please produce a step-by-step reasoning content including perception, prediction and planning, following the style shown in the example.\n",
    "    \n",
    "    '''\n",
    "    system_content = [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "    user_content = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "    assistant_content = [{\"type\": \"text\", \"text\": \"Step-by-Step reasoning:\"}]\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Camera Views:\\n\"})\n",
    "    if use_base64:\n",
    "        # Insert base64 data (GPT-3.5/4 standard models typically cannot decode, but let's show it anyway)\n",
    "        for view, image_path in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            base64_image = encode_image(image_path)\n",
    "            user_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "            })                    \n",
    "    else:\n",
    "        # Insert textual descriptions\n",
    "        for view, desc in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            user_content.append({\"type\": \"text\", \"text\": desc})\n",
    "    \n",
    "\n",
    "    return system_content, user_content, assistant_content\n",
    "\n",
    "def generate_reasoning_and_action(client, camera_dict, use_base64=False):\n",
    "    \"\"\"\n",
    "    1. Optionally encode images or prepare text descriptions.\n",
    "    2. Build a prompt using that info.\n",
    "    3. Call OpenAI GPT to generate chain-of-thought reasoning + final action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the prompt\n",
    "    system_content, user_content, assistant_content = build_autonomous_driving_prompt(\n",
    "        camera_info_dict=camera_dict,\n",
    "        use_base64=use_base64\n",
    "    )\n",
    "\n",
    "    # Make the ChatCompletion call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    # Extract the GPT response\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'object_key': '<c1,CAM_BACK,1088.3,497.5>', 'Category': 'Vehicle', 'Status': 'Moving', 'Visual_description': 'Brown SUV.', '2d_bbox': [966.6, 403.3, 1224.1, 591.7]}, {'object_key': '<c2,CAM_BACK,864.2,468.3>', 'Category': 'Vehicle', 'Status': 'Moving', 'Visual_description': 'Black sedan.', '2d_bbox': [816.7, 431.6, 917.2, 505.0]}, {'object_key': '<c3,CAM_FRONT,1043.2,82.2>', 'Category': 'Traffic element', 'Status': None, 'Visual_description': 'Green light.', '2d_bbox': [676.4, 0.0, 1452.6, 171.5]}]\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = str(results[0]['objects']['objects'])\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CAM_FRONT': '../nuscenes/samples/CAM_FRONT/n008-2018-09-18-13-10-39-0400__CAM_FRONT__1537291002262404.jpg',\n",
       " 'CAM_FRONT_LEFT': '../nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-13-10-39-0400__CAM_FRONT_LEFT__1537291002254799.jpg',\n",
       " 'CAM_FRONT_RIGHT': '../nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-13-10-39-0400__CAM_FRONT_RIGHT__1537291002270482.jpg',\n",
       " 'CAM_BACK': '../nuscenes/samples/CAM_BACK/n008-2018-09-18-13-10-39-0400__CAM_BACK__1537291002287558.jpg',\n",
       " 'CAM_BACK_LEFT': '../nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-13-10-39-0400__CAM_BACK_LEFT__1537291002297405.jpg',\n",
       " 'CAM_BACK_RIGHT': '../nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-13-10-39-0400__CAM_BACK_RIGHT__1537291002278113.jpg'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = results[1]['objects']['image_path']\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Detected Objects & Status:**\\n\\n1. **Front View (CAM_FRONT):**\\n   - Green traffic light visible.\\n   - Wet road conditions due to rain.\\n   - No immediate vehicles or pedestrians in the intersection.\\n\\n2. **Front-Left View (CAM_FRONT_LEFT):**\\n   - Red traffic light for cross traffic.\\n   - No vehicles or pedestrians approaching from the left.\\n\\n3. **Front-Right View (CAM_FRONT_RIGHT):**\\n   - Red traffic light for cross traffic.\\n   - No vehicles or pedestrians approaching from the right.\\n\\n4. **Back View (CAM_BACK):**\\n   - A vehicle directly behind, maintaining a close distance.\\n   - Wet road conditions.\\n\\n5. **Back-Left View (CAM_BACK_LEFT):**\\n   - Parked vehicles and postal trucks visible.\\n   - No moving vehicles in the immediate vicinity.\\n\\n6. **Back-Right View (CAM_BACK_RIGHT):**\\n   - Clear sidewalk with no pedestrians or cyclists.\\n   - No immediate obstacles.\\n\\n**Future Movement Prediction:**\\n\\n- **Front:** The green light suggests it is safe to proceed, but caution is needed due to wet conditions.\\n- **Back:** The vehicle behind may maintain its close distance, especially in stop-and-go traffic.\\n- **Left and Right (Cross Traffic):** Likely to remain stopped due to red lights.\\n\\n**Action Planning:**\\n\\n- **Proceed with Caution:** Advance through the intersection while maintaining awareness of the wet road, which may increase stopping distance.\\n- **Monitor Rear Vehicle:** Keep an eye on the vehicle behind using rearview mirrors, especially if needing to stop suddenly.\\n- **Maintain Speed and Distance:** Ensure a safe following distance once through the intersection, considering the vehicle behind.\\n- **Stay Alert:** Keep an eye on potential sudden movements from cross traffic in case of signal changes.\\n\\n**Conclusion:**\\n\\nProceed through the intersection cautiously, maintaining awareness of both the wet road conditions and the vehicle behind. Ensure smooth and controlled acceleration to prevent slipping.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generate_reasoning_and_action(client, image_path, use_base64=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_to_multiple_choice(client, question, answer):\n",
    "    \"\"\"\n",
    "    Sends a request to GPT to convert a single Q/A into a 5-option multiple-choice question.\n",
    "    Returns a tuple: (revised_question_string, correct_answer_letter).\n",
    "    \"\"\"\n",
    "\n",
    "    # You can fine-tune this system prompt if desired\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant that rewrites autonomous driving Q/A into multiple-choice format. \"\n",
    "        \"You must produce exactly one correct option that matches the original answer, \"\n",
    "        \"and four distractors that are plausible but incorrect. \"\n",
    "        \"Return output as a Python tuple of two elements: \"\n",
    "        \"(\\\"Revised MC question\\\", \\\"CorrectAnswerLetter\\\"). \"\n",
    "        \"Only one letter from A,B,C,D,E should be correct.\"\n",
    "    )\n",
    "\n",
    "    # User message with the raw Q/A. We instruct GPT how to format\n",
    "    user_message = f\"\"\"\n",
    "        Original Question: {question}\n",
    "        Original Answer: {answer}\n",
    "\n",
    "        Instructions:\n",
    "        1. Convert the Q/A into a multiple-choice question with exactly 5 options (A, B, C, D, E).\n",
    "        2. Only one option should be correct(can be anyone from A to E), reflecting the original answer.\n",
    "        3. Provide 4 other distractor options that are different from the correct one.\n",
    "        4. Format the final output strictly as a Python tuple with two elements:\n",
    "        ( \"<multiline MC question>\", \"<single letter denoting correct answer>\" )\n",
    "        5. The MC question should look like this:\n",
    "\n",
    "        <QUESTION>\n",
    "        A) ...\n",
    "        B) ...\n",
    "        C) ...\n",
    "        D) ...\n",
    "        E) ...\n",
    "\n",
    "        6. The second element of the tuple is the letter of the correct choice, e.g. \"B\".\n",
    "        7. Do not add extra text or explanation outside the tuple.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # or another ChatGPT-compatible model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=400,\n",
    "    )\n",
    "\n",
    "    # The assistant's reply should be a string that looks like: \n",
    "    # ( \"Predict the behavior of the ego vehicle...\\nA) ...\\nB) ...\\nC) ...\\nD) ...\\nE) ...\", \"C\" )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ego vehicle is going straight. The ego vehicle is driving fast.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_id = results[0]['scene_id']\n",
    "frame_id = results[0]['frame_id']\n",
    "QA = index_dict[scene_id]['key_frames'][frame_id]['QA']['behavior'][0]\n",
    "question = QA['Q']\n",
    "answer = QA['A']\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the behavior of the ego vehicle?A) The ego vehicle is turning left.B) The ego vehicle is stopping.C) The ego vehicle is going straight and driving fast.D) The ego vehicle is reversing.E) The ego vehicle is changing lanes to the right.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "new_QA = qa_to_multiple_choice(client, question, answer)\n",
    "new_QA = new_QA.replace('```','').replace('python','').replace('\\n','').replace('\\\\n','')\n",
    "new_QA = ast.literal_eval(new_QA)\n",
    "new_Q = new_QA[0]\n",
    "new_A = new_QA[1]\n",
    "new_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_verify_prompt(\n",
    "    reasoning_context,\n",
    "    question\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a text prompt for GPT by either inserting\n",
    "    base64-encoded images or textual descriptions.\n",
    "    \"\"\"\n",
    "    # Start with a short context\n",
    "    system_prompt = '''\n",
    "        You are an advanced autonomous driving assistant that uses provided reasoning context\n",
    "        to answer multiple-choice questions. You should pick the single best correct option.\"\n",
    "    '''\n",
    "\n",
    "    # The user message includes demonstration data + the new scenario\n",
    "    user_prompt = '''\n",
    "    Below is data from an autonomous driving scenario. You are provided with:\n",
    "    1) Reasoning context derived from a real driving scenario.\n",
    "    2) A multi-choice question asking about the correct and safe driving action.\n",
    "\n",
    "    Instruction\n",
    "    1) Please analyze the reasoning context carefully, then select the single best answer (A, B, C, D, or E).\n",
    "    2) Only output a single letter\n",
    "\n",
    "    \n",
    "    '''\n",
    "    system_content = [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "    user_content = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "    assistant_content = [{\"type\": \"text\", \"text\": \"The single letter answer is:\"}]\n",
    "    \n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Reasoning context:\\n\"})\n",
    "    user_content.append({\"type\": \"text\", \"text\": reasoning_context})\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Multi-choice question:\\n\"})\n",
    "    user_content.append({\"type\": \"text\", \"text\": question})\n",
    "    \n",
    "\n",
    "    return system_content, user_content, assistant_content\n",
    "\n",
    "def generate_final_action(client, reasoning_context, question):\n",
    "    \"\"\"\n",
    "    1. Optionally encode images or prepare text descriptions.\n",
    "    2. Build a prompt using that info.\n",
    "    3. Call OpenAI GPT to generate chain-of-thought reasoning + final action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the prompt\n",
    "    system_content, user_content, assistant_content = build_verify_prompt(\n",
    "        reasoning_context,\n",
    "        question\n",
    "    )\n",
    "\n",
    "    # Make the ChatCompletion call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=4\n",
    "    )\n",
    "\n",
    "    # Extract the GPT response\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = generate_final_action(client, output, new_Q)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_refine_prompt(\n",
    "    camera_info_dict,\n",
    "    reasoning_context,\n",
    "    use_base64=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a text prompt for GPT by either inserting\n",
    "    base64-encoded images or textual descriptions.\n",
    "    \"\"\"\n",
    "    # Start with a short context\n",
    "    system_prompt = '''\n",
    "        You are an advanced autonomous driving assistant specialized in \n",
    "        concise reasoning about camera images to determine the correct driving action.\n",
    "    '''\n",
    "\n",
    "    # The user message includes demonstration data + the new scenario\n",
    "    user_prompt = '''\n",
    "    Below is data from an autonomous driving scenario. You are provided with:\n",
    "    1) Six camera images (front-left, front, front-right, back-left, back, back-right).\n",
    "    2) A current chain-of-thought reasoning.\n",
    "\n",
    "    Goal: Produce a shorter, more concise version of the reasoning that only includes details \n",
    "    necessary for deriving the final driving action. Remove unnecessary analysis, extraneous \n",
    "    tangents, or repeated points. Rephrase any sentences to be more succinct while preserving \n",
    "    meaning.\n",
    "\n",
    "    Instructions:\n",
    "    1. Review the camera images and the current reasoning.\n",
    "    2. Delete or omit irrelevant details that do not influence the final driving action.\n",
    "    3. Rephrase what's left so it's concise but still logically consistent.\n",
    "    \n",
    "    '''\n",
    "    system_content = [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "    user_content = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "    assistant_content = [{\"type\": \"text\", \"text\": \"Concise reasoning:\"}]\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Camera Views:\\n\"})\n",
    "    if use_base64:\n",
    "        # Insert base64 data (GPT-3.5/4 standard models typically cannot decode, but let's show it anyway)\n",
    "        for view, image_path in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            base64_image = encode_image(image_path)\n",
    "            user_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "            })                    \n",
    "    else:\n",
    "        # Insert textual descriptions\n",
    "        for view, desc in camera_info_dict.items():\n",
    "            user_content.append({\"type\": \"text\", \"text\": f\"{view}:\"})\n",
    "            user_content.append({\"type\": \"text\", \"text\": desc})\n",
    "\n",
    "    user_content.append({\"type\": \"text\", \"text\": \"Current reasoning chain:\\n\"})\n",
    "    user_content.append({\"type\": \"text\", \"text\": reasoning_context})\n",
    "    \n",
    "\n",
    "    return system_content, user_content, assistant_content\n",
    "\n",
    "def generate_concise_reasoning(client, camera_dict, reasoning_context, use_base64=False):\n",
    "    \"\"\"\n",
    "    1. Optionally encode images or prepare text descriptions.\n",
    "    2. Build a prompt using that info.\n",
    "    3. Call OpenAI GPT to generate chain-of-thought reasoning + final action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the prompt\n",
    "    system_content, user_content, assistant_content = build_refine_prompt(\n",
    "        camera_info_dict=camera_dict,\n",
    "        reasoning_context=reasoning_context,\n",
    "        use_base64=use_base64\n",
    "    )\n",
    "\n",
    "    # Make the ChatCompletion call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=256\n",
    "    )\n",
    "\n",
    "    # Extract the GPT response\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Camera Analysis:**\\n\\n1. **Front View (CAM_FRONT):** \\n   - Green light, wet road, no vehicles/pedestrians.\\n\\n2. **Front-Left (CAM_FRONT_LEFT) and Front-Right (CAM_FRONT_RIGHT):** \\n   - Cross traffic stopped at red lights, no approaching vehicles/pedestrians.\\n\\n3. **Back View (CAM_BACK):**\\n   - Vehicle close behind, wet conditions.\\n\\n4. **Left and Right Rear Views:** \\n   - Clear of moving vehicles, no immediate obstacles.\\n\\n**Action Plan:**\\n\\n- Proceed through the intersection cautiously due to green light and wet roads.\\n- Monitor the vehicle behind and ensure a safe distance.\\n- Maintain smooth acceleration to prevent slipping.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concise_reasoning = generate_concise_reasoning(client, image_path, output, use_base64=True)\n",
    "concise_reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = generate_final_action(client, concise_reasoning, new_Q)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert action == new_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n401 Client Error. (Request ID: Root=1-67f5a78e-60f95e072e4bbcab7595e853;945ac74c-38fe-4291-839a-3b02a936622b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/transformers/utils/hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/file_download.py:961\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/file_download.py:1068\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/file_download.py:1596\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1593\u001b[0m ):\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1484\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/file_download.py:1401\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/file_download.py:285\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 285\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    308\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 309\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:426\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    423\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m     )\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-67f5a78e-60f95e072e4bbcab7595e853;945ac74c-38fe-4291-839a-3b02a936622b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 21\u001b[0m\n\u001b[1;32m      9\u001b[0m user_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    Below is data from an autonomous driving scenario. You are provided with:\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    1) Reasoning context derived from a real driving scenario.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    2) Only output a single letter\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     19\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m messages \u001b[38;5;241m=\u001b[39m user_prompt\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Reasoning context:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mconcise_reasoning\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Multi-choice question:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mnew_Q\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m The single letter answer is:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m pipe(messages)\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/transformers/pipelines/__init__.py:851\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 adapter_path \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    849\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 851\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    856\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1112\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1109\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1110\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1112\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1114\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/transformers/configuration_utils.py:590\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/transformers/configuration_utils.py:649\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/transformers/utils/hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/anaconda3/envs/llavaverify/lib/python3.10/site-packages/transformers/utils/hub.py:481\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    484\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, LocalEntryNotFoundError):\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n401 Client Error. (Request ID: Root=1-67f5a78e-60f95e072e4bbcab7595e853;945ac74c-38fe-4291-839a-3b02a936622b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "import transformers\n",
    "import torch\n",
    "system_prompt = '''\n",
    "        You are an advanced autonomous driving assistant that uses provided reasoning context\n",
    "        to answer multiple-choice questions. You should pick the single best correct option.\n",
    "    '''\n",
    "\n",
    "user_prompt = '''\n",
    "    Below is data from an autonomous driving scenario. You are provided with:\n",
    "    1) Reasoning context derived from a real driving scenario.\n",
    "    2) A multi-choice question asking about the correct and safe driving action.\n",
    "\n",
    "    Instruction\n",
    "    1) Please analyze the reasoning context carefully, then select the single best answer (A, B, C, D, or E).\n",
    "    2) Only output a single letter\n",
    "    '''\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "pipe = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
    "messages = user_prompt+'\\n Reasoning context:'+concise_reasoning+'\\n Multi-choice question:'+new_Q+'\\n The single letter answer is:'\n",
    "\n",
    "out = pipe(messages)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f078dc2805cd4a8a933983e8fc57d2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
